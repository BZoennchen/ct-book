{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unsigned-logging",
   "metadata": {},
   "source": [
    "# Strukturwissenschaften\n",
    "\n",
    "## Nachrichtentechnik \n",
    "\n",
    "1948 griff Claude E. Shannon den physikalischen Begriff der *Entropie* auf und übetrug Ihn auf die Nachrichtenübertragung.\n",
    "Sein Modell prägte die Informatik über viele Jahre und ist heute noch relevant.\n",
    "Es ist äußerst wichtig zu verstehen, dass es Shannon um die *Informationsübertragung* ging.\n",
    "Sie ist für sein Modell von zentraler Bedeutung.\n",
    "So schreibt er in {cite}`shannon:1948`:\n",
    "\n",
    "> Frequently the messages have meaning; that is they refer to or are correleted according to some system with certain physical or conceptual entities. These **semantic aspects of communication are irrelevant** to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. -- Claude E. Shannon\n",
    "\n",
    "Shannon war sich bewusst, dass zur Information auch Ihre Bedeutung (Semantik) gehört, doch befand er diese für die *Informationsübertragung* über ein Medium als irrelevant.\n",
    "\n",
    "Die Übertragung besteht dabei aus mehreren Teilen:\n",
    "\n",
    "1. Eine (Informations)-**Quelle** versendet eine **Nachricht**.\n",
    "2. Diese Nachricht wird durch einen **Sender** in ein **Signal** codiert und in einen **Kanal** eingespeist.\n",
    "3. Der **Kanal** ist das Medium der Übertragung, beispielsweise ein Netzwerkkabel. Bei dieser Übertragung kann es zu einer Störung durch eine **Störquelle** kommen. Unter Umständen ist das **erhaltene Signal** gestört.\n",
    "4. Ein **Empfänger** empfängt dieses möglicherweise gestörte **erhaltene Signal**, decodiert es und leitet es an das **Ziel** weiter.\n",
    "5. Die decodierte **Nachricht** kommt bei ihrem Ziel an.\n",
    "\n",
    "```{figure} ../../figs/information/shannon-messages.png\n",
    "---\n",
    "width: 600px\n",
    "name: fig-shannon-messages\n",
    "---\n",
    "Shannon's Schema der Nachrichtenübertragung bzw. Kommunikation.\n",
    "```\n",
    "\n",
    "In diesem Modell ist die Nachricht/Symbolfolge/Zeichenfolge eine *Information* und der *Informationsgehalt* ist umso größer je mehr mögliche Nachrichten es gibt.\n",
    "\n",
    "> If the number of [possible] messages [...] is finite then this number [...] can be regarded as a measure of the information produced when one message [... from the set of possible messages], all\n",
    "choices being equally likely -- Claude E. Shannon\n",
    "\n",
    "Betrachten wir die folgenden beiden Zeichenfolgen:\n",
    "\n",
    "1. Prüfung bestanden\n",
    "2. brüeung Pfnsanedt\n",
    "\n",
    "Nach Shannon's Definition enthalten beide Symbolketten/Zeichenfolgen gleich viel Information.\n",
    "Die Anzahl der [Bits](def-bit), welche wir bräuchten um die beiden Nachrichten auf einem Kanal zu übertragen ist dieselbe.\n",
    "Intuitiv würden wir sagen, dass die erste Nachricht mehr Information beinhaltet.\n",
    "Dies liegt daran, dass wir automatisch die [Semantik](def-semantik) (Bedeutung) einer Zeichenfolge betrachten.\n",
    "Shannon's Definition ist jedoch rein [syntaktisch](def-syntax), denn Maschinen fehlt (bis heute) die Eigenschaft der semantischen Betrachtung.\n",
    "In der Informatik gibt es deshalb (noch) keine befriedigende Definition von Information, die auch der Bedeutung der betrachteten Information gerecht wird und selbst die syntaktischen Definitionen sind nicht besonders aussagekräftig.\n",
    "Das ist doch sehr erstaunlich, dass die Informatik, also jene Wissenschaft, die sich als ihr zentrales Forschungsobjekt, mit der Information beschäftigt, keine wirklich aussagekräftige Definition von genau jener Information kennt.\n",
    "\n",
    "Gehen wir zurück zu unserem Einführungsbeispiel in der U-Bahn.\n",
    "Falls der Wetterbericht Ihnen lediglich mitteilen kann ob es regnet oder nicht, so ist der Informationsgehalt, dass es regnet gering.\n",
    "Wenn aber der Wetterbericht Ihnen unterschiedliche Arten von Regen oder andere Wetterlagen berichten kann, so trägt eine dieser konkreten Wetterlagen einen großen Informationsgehalt in sich.\n",
    "Auch hier sehen wir aber die schwäche dieser Definition: Ob es draußen 25 Grad oder 25.00001 Grad hat, ist Ihnen höchst wahrscheinlich gleichgültig.\n",
    "Doch ein Wetterbericht der Ihnen die Temperatur auf fünf Nachkommastellen genau berichtet, liefert nach Shannon's Definition einen viel höheren Informationsgehalt als ein Dienst, der bei der ersten Nachkommastelle rundet.\n",
    "\n",
    "Einer der Vorzüge der (strukturwissenschaftlichen) Definition ist jedoch, dass wir ein Maß für den *Informationsgehalt* definieren können.\n",
    "Shannon misst den Informationsgehalt einer Nachricht über die *Entropie*, welche uns in der Physik schon über den Weg gelaufen ist.\n",
    "Die Entropie in der Informatik ist ein Maß für den *mittleren Informationsgehalt* einer *Nachricht*.\n",
    "\n",
    "Betrachten wir eine bestimmte endliche Menge an Zeichen die bei einer Übertragung auftauchen können.\n",
    "Diese Menge bezeichnet man üblicherweise als Alphabet $\\Sigma$.\n",
    "Sei nun $a \\in \\Sigma$ ein Zeichen.\n",
    "Wie wahrscheinlich ist es, dass ein Zeichen $X$ einer Nachricht gleich $a$ ist, d.h. wie groß ist\n",
    "\n",
    "$$P(X = \\sigma) = p_\\sigma ?$$\n",
    "\n",
    "Wenn jedes Zeichen mit gleicher Wahrscheinlichkeit in einer Nachricht auftritt, dann ist \n",
    "\n",
    "$$P(X = \\sigma) = \\frac{1}{N},$$\n",
    "\n",
    "wobei $N = |\\Sigma|$ gleich der Anzahl der Zeichen im Alphabet $\\Sigma$ ist.\n",
    "\n",
    "Wie viel Information steckt dann in einem Zeichen?\n",
    "Anders gefragt: Wie viel Unsicherheit eliminiert ein Zeichen oder wie viel Ja/Nein Fragen müssten wir stellen um auf das übertragene Zeichen $\\sigma$ zu kommen?\n",
    "Die wenigsten Fragen müssen wir stellen wenn wir mit der sog. [binären Suche](def-binary-search) vorgehen, siehe auch die Übung [Sprechen in der Taucherglocke](sec-dive-bell).\n",
    "Dabei verringert sich bei jeder Frage die Anzahl der Möglichkeiten um die Hälfte.\n",
    "Mit dieser Strategie benötigen wir\n",
    "\n",
    "$$\\log_2\\left( N \\right) = \\log_2\\left( \\frac{1}{p_\\sigma} \\right)$$\n",
    "\n",
    "Fragen.\n",
    "Da in einer normalen Sprache jedoch nicht jedes Zeichen mit der gleichen Wahrscheinlichkeit auftritt, berücksichtigen wir diese ebenfalls.\n",
    "Jedes Zeichen $\\sigma_i$ kann eine andere Auftrittswahrscheinlichkeit $p_i$ besitzen.\n",
    "Der *Informationsgehalt* $I$, der dann in dem $i$-ten Symbol $\\sigma_i$ steckt, ist dann \n",
    "\n",
    "$$I(\\sigma_i) = \\log_2\\left( \\frac{1}{p_i} \\right) = -\\log_2(p_i).$$\n",
    "\n",
    "```{admonition} Entropie (Informatik)\n",
    ":name: def-entropie\n",
    ":class: definition\n",
    "\n",
    "Sei $X$ eine Zufallsvariable einer gedächtnislosen Quelle über einem endlichen, aus Zeichen bestehenden Alphabet $\\Sigma = \\left\\{ \\sigma_1, \\ldots, \\sigma_m \\right\\}$.\n",
    "Sei $p_i = P(X = \\sigma_i)$ die Wahrscheinlichkeit, dass die Zufallsvariable $X$ das Zeichen $\\sigma_i$ annimmt.\n",
    "Dann ist \n",
    "\n",
    "$$I(\\sigma_i) = \\log_2(1/p_i) = - \\log_2(p_i)$$\n",
    "\n",
    "der **Informationsgehalt des Zeichens** (notwendige Binärbits um $1 / p_i$ Ereignisse zu unterscheiden).\n",
    "\n",
    "Die **Entropie eines Zeichens** ist definiert als der Erwartungswert des Informationsgehalts:\n",
    "\n",
    "$$H_1 = E[I] = \\sum^m_{i=1} p_i I(\\sigma_i) = - \\sum^m_{i=1} p_i \\log_2(p_i)$$\n",
    "\n",
    "Die **Entropie** $H_n$ **für Wörter** $w \\in \\Sigma^n$ der Länge $n$ ergibt sich durch\n",
    "\n",
    "$$H_n = - \\sum_{w \\in \\Sigma^n} p_w \\log_2(p_w),$$\n",
    "\n",
    "wobei $p_w = P(X = w)$ die Wahrscheinlichkeit ist, mit der das Wort $w$ auftritt.\n",
    "\n",
    "Die **Entropie** $H$ ist der Limes $n \\rightarrow \\infty$\n",
    "\n",
    "$$H = \\lim_{n \\rightarrow \\infty} \\frac{H_n}{n}.$$\n",
    "```\n",
    "\n",
    "Lassen Sie sich nicht von den mathematischen Symbolen abschrecken.\n",
    "Kurz gesagt: je größer die Gewissheit, desto kleiner ist der Informationsgehalt.\n",
    "Information führt zur Beseitigung von Unsicherheit.\n",
    "Je mehr Unsicherheit beseitigt wird, desto größer ist der Informationsgehalt!\n",
    "\n",
    "Zwischen der Entropie eines Zeichens und den notwendigen Bits einer Nachricht gibt es einen schönen Zusammenhang:\n",
    "Ist ein bestimmtes Alphabet $\\Sigma$ mit all den Auftrittswahrscheinlichkeiten seiner Zeichen gegeben, so benötigen wir\n",
    "\n",
    "$$m \\cdot H_1$$\n",
    "\n",
    "[Bits](def-bit) um eine Nachricht mit $m$ Zeichen zu übertragen.\n",
    "\n",
    "Das Konzept der *Entropie* wird schnell anhand eines Beispiels klar.\n",
    "Nehmen wir den Münzwurf mit einer perfekten Münze.\n",
    "Werfen Sie die Münze einmal, so ist der Informationsgehalt $H_1 = 1$, denn die Wahrscheinlichkeit für Kopf oder Zahl liegt bei $0.5$ und daraus ergibt sich:\n",
    "\n",
    "$$H_1 = 0.5 \\cdot \\log_2(2) + 0.5 \\cdot \\log_2(2) = 0.5 + 0.5 = 1$$\n",
    "\n",
    "Die Informationsquelle ist der Münzwurf, die Nachricht das Ergebnis des Wurfes und der Empfänger sind wir, die den Münzwurf beobachten.\n",
    "\n",
    "```{exercise} Entropie\n",
    ":label: entropie-exercise\n",
    "Welche Entropie $H$ hat die Nachricht die von einer Informationsquelle stammt, welche durchgehend nur $1$ sendet?\n",
    "```\n",
    "\n",
    "```{solution} entropie-exercise\n",
    ":label: entropie-solution\n",
    ":class: dropdown\n",
    "Da für jedes Wort $w$ die Wahrscheinlichkeit gleich $1$ ist, d.h. $P(X = w) = 1$, folgt $H = 0$.\n",
    "```\n",
    "\n",
    "Lassen Sie uns ein weiteres Beispiel betrachten.\n",
    "Stellen Sie sich vor, Sie hätten eine Informationsquelle die fortwährend die Folge $0,1,0,1,0,1, \\ldots$ ausspuckt.\n",
    "Für $H_1$ ergibt sich $H_1 = 1$, denn wie beim Münzwurf besteht eine Wahrscheinlichkeit von $0.5$ ob wir eine $0$ oder $1$ für ein Zeichen an einer zufälligen Stelle erhalten.\n",
    "Das ändert sich jedoch nicht wenn wir die Wortlänge auf $2$ erhöhen. Es ergibt sich $H_2 = 1$, denn mit Wahrscheinlichkeit $0.5$ sehen wir entweder $1,0$ oder $0,1$.\n",
    "Wäre die Folge rein zufällig, wäre $H_2 = 2$, denn mit Wahrscheinlichkeit $0.25$ sähen wir $0,0$ oder $0,1$ oder $1,0$ oder $1,1$.\n",
    "Es ergibt sich:\n",
    "\n",
    "$$H_2 = 4 \\cdot 0.25 \\cdot \\log_2(4) = \\log_2(4) = 2$$\n",
    "\n",
    "Für unsere Folge $0,1,0,1,0,1, \\ldots$ entscheidet das erste Zeichen über alle weitere Zeichen, egal wie lang das Wort ist, welches wir betrachten.\n",
    "Somit ist die Entropie $H_n = 1$ und damit\n",
    "\n",
    "$$H = \\lim_{n \\rightarrow \\infty} \\frac{H_n}{n} = \\lim_{n \\rightarrow \\infty} \\frac{1}{n} = 0$$\n",
    "\n",
    "Der Informationsgehalt ist gering und wir könnten mit deutlich weniger Zeichen, die gleiche Information übertragen!\n",
    "Die rein zufällige Folge aus $0$ und $1$ steht im Gegensatz dazu und enthält sehr viel Information, denn\n",
    "\n",
    "$$H_n = \\log(2^n) = n$$\n",
    "\n",
    "und damit gilt\n",
    "\n",
    "$$H = \\lim_{n \\rightarrow \\infty} \\frac{H_n}{n} = \\lim_{n \\rightarrow \\infty} \\frac{n}{n} = 1.$$\n",
    "\n",
    "Bei Shannon's Informationsbegriff gehen wir davon aus, dass Information in kleinste Informationsteile ([Bits](def-bit)) zerlegbar sind.\n",
    "Zudem ist die Summe dieser Teilchen die Information selbst.\n",
    "Beim Zusammensetzen der Einzelteile, also der Kombination von Symbolen, entsteht demnach keine neue Qualität, welche eben nicht auf die einzelnen Symbole reduziert werden kann.\n",
    "\n",
    "```{admonition} Information (Informatik)\n",
    ":class: remark\n",
    "Eine *Information* ist eine übertragene Nachricht und kann als die Summe ihrer Einzelteile betrachtet werden.\n",
    "```\n",
    "\n",
    "Shannon's Informationsbegriff ignoriert jedwede [Semantik](def-semantik) und verdinglicht die Information, dennoch ist er aus technischer Sicht sehr nützlich.\n",
    "Sie hilft uns zu bestimmen wie viel Zeichen ([Syntax](def-syntax)) notwendig sind, um etwas auszudrücken.\n",
    "Eine hohe [Entropie](def-entropie) bedeutet in diesem Zusammenhang, dass wir unsere Zeichen sparsam nutzen.\n",
    "Außerdem kann die Entropie als Informationsgehalt aus Sicht der digitalen Maschine verstanden werden, da sie keinerlei Semantik a priori kennt.\n",
    "Zwar kann der Computer, zum Beispiel zwei Binärzahlen addieren, doch hat er keinerlei Vorstellung vom Gegenstand der Zahl.\n",
    "Alle Manipulationen sind syntaktischer Natur, d.h. eine Manipulation von Zeichen.\n",
    "\n",
    "```{admonition} Formale Methoden\n",
    ":class: remark\n",
    "Formale Methoden sind unter anderem eine Anstrengung um wesentliche semantische Aspekte von Sprache und Denken auf regelgeleitete, rein syntaktische Symbolmanipulation zurückzuführen.\n",
    "```\n",
    "\n",
    "## Informationstheorie\n",
    "\n",
    "Verlassen wir die Sicht der Nachrichtenübertragung, so hilft uns der klassische Informationsbegriff von Shannon nicht sonderlich weiter.\n",
    "Wie etwa unterscheidet sich der *Informationsgehalt* von einer zufälligen Bitfolge der Länge $n$ zu einer angenäherten Darstellung der Kreiszahl $\\pi$ beschränkt auf $n$ [Bits](def-bit)?\n",
    "In Shannon's Theorie agiert ein Zusammenschluss von Akteuren (Sender, Empfänger, Kanal, Nachricht).\n",
    "Um seinen Informationsbegriff auf eine einzelne Zeichenkette zu nutzten, müssten uns irgendwie einen künstlichen Nachrichtenkanal und Sender konstruieren.\n",
    "\n",
    "Anstatt der Perspektive aus der Nachrichtenübertragung betrachtet die algorithmische Informationstheorie mit der Kolmogorow-Komplexität die algorithmische Beschreibung einer Zeichenfolge.\n",
    "Kurz gesagt: Die Kolmogorow-Komplexität einer Zeichenkette ist die Länger ihrer kürzesten Beschreibung.\n",
    "Jene Beschreibung ist nichts anderes als ein Algorithmus!\n",
    "Je kürzer der Algorithmus bzw. die Beschreibung, desto weniger Information enthält die durch ihn erzeugte/beschriebene Zeichenkette.\n",
    "\n",
    "```{admonition} Kolmogorow-Komplexität\n",
    ":name: def-kolmogorow-complexity\n",
    ":class: definition\n",
    "\n",
    "Für eine Programmiersprache $U$, z.B. beschrieben als [universelle Turingmaschine](sec-utm), ist der *Informationsgehalt* oder *Kolmogorow-Komplexität* $C_U(w)$ einer Zeichenfolge $w$ definiert als die Bitlänge des kürzesten Programms $T$ (z.B. [Turingmaschine](info-turingmaschine)), das ohne weitere Eingabe die Zeichenfolge $w$ als Ausgabe erzeugt. \n",
    "Mathematisch ausgedrückt:\n",
    "\n",
    "$$C_U(w) = \\min\\limits_{\\alpha_T} \\left\\{ |\\alpha_T| : U(\\alpha_T) = T() = w\\right\\},$$\n",
    "\n",
    "wobei $T()$ bedeutet, dass die Turingmaschine $T$, welche durch $\\alpha_T$ beschrieben ist, ohne Eingabe startet.\n",
    "$|\\alpha_T|$ ist die Länge der Beschreibung der Turingmaschine $T$.\n",
    "```\n",
    "\n",
    "Nehmen wir an wir möchten folgende Zeichenkette\n",
    "\n",
    "$$\\text{abababababababababab}$$\n",
    "\n",
    "also 10 mal die Zeichen $\\text{ab}$ *beschreiben*. \n",
    "In ``Python`` können wir das mit folgendem Algorithmus vollziehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "improving-seeking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abababababababababab'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abababababababababab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-dryer",
   "metadata": {},
   "source": [
    "Dieser Algorithmus benötigt insgesamt 22 Zeichen.\n",
    "Allerdings ist das nicht das kürzeste Programm.\n",
    "Folgender Algorithmus/Beschreibung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "allied-sunday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abababababababababab'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ab'*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-intervention",
   "metadata": {},
   "source": [
    "benötigt lediglich 7 Zeichen und erzeugt die gleiche Zeichenkette.\n",
    "Wie sieht es nun aus wenn wir aus dem mittleren $\\text{a}$ ein $\\text{b}$ machen?\n",
    "Wenn wir also\n",
    "\n",
    "$$\\text{ababababaaababababab}$$\n",
    "\n",
    "beschreiben wollen?\n",
    "Folgendes ``Python``-Programm erzeugt die neue Zeichenkette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "violent-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ababababaaababababab'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ab'*4+'aa'+5*'ab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-sussex",
   "metadata": {},
   "source": [
    "Wären wir uns sicher, dass die angegebenen Algorithmen die kürzesten Beschreibungen sind, so können wir sagen dass $\\text{abababababababababab}$ weniger Information enthält als $\\text{ababababaaababababab}$.\n",
    "Löschen wir ein $\\text{a}$ aus der ursprünglichen Zeichenkette, z.B. $\\text{ababbababababababab}$ so müssen wir den ``Python``-Code wie folgt anpassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supposed-wrist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ababbababababababab'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ab'*2+'b'+7*'ab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-tsunami",
   "metadata": {},
   "source": [
    "Obwohl die Zeichenkette kürzer ist, enthält sie mehr Information.\n",
    "Kürzere Zeichenketten können nach dieser Definition mehr Informationen enthalten als längere, entscheidend ist deren *Beschreibungslänge*.\n",
    "\n",
    "Die kürzeste Beschreibung kann nicht länger als die nicht-algorithmische Beschreibung der Zeichenkette sein.\n",
    "Das bedeutet, wir können immer die Zeichenkette in Form einer Programmiersprache niederschreiben, so wie wir das ganz zu Beginn getan haben.\n",
    "\n",
    "```{admonition} Algorithmische Zufälligkeit\n",
    ":name: def-compressible\n",
    ":class: definition\n",
    "Falls die Länge der kürzesten Beschreibung einer Zeichenkette größer oder gleich der Länge der Zeichenkette selbst ist, dann ist nennen wir diese Zeichenkette *algorithmisch zufällig* und *unkomprimierbar*.\n",
    "```\n",
    "\n",
    "Problematisch an der [Kolmogorow-Komplexität](def-kolmogorow-complexity) und im Gegensatz zur [Entropie](def-entropie) gilt, dass wir sie im Allgemeinen nicht berechnen können.\n",
    "\n",
    "```{admonition} Unberechenbarkeit der Kolmogorow-Komplexität\n",
    ":name: def-uncomputable\n",
    ":class: theorem\n",
    "Sei eine beliebige Zeichenkette $w$ und $U$ eine Programmiersprache gegeben, so ist die Kolmogorow-Komplexität $C_U(w)$ im Allgemeinen **nicht** [berechenbar](def-computable).\n",
    "```\n",
    "\n",
    "Haben wir eine bestimmte Beschreibung für eine Zeichenkette, könnte es immer eine noch kürzere Beschreibung geben.\n",
    "Wir können jedoch immer eine Obergrenze für die Kolmogorow-Komplexität bestimmen. \n",
    "\n",
    "Außerdem kann man zeigen, dass für zwei universelle Turingmaschinen $U$ und $U'$ sich $C_U(w)$ und $C_{U'}(w)$ für alle $w$ maximal um eine additive Konstante unterscheiden (*Theorem der Invarianz*).\n",
    "Das geht aus der Komplexitätstheorie hervor.\n",
    "Somit ist die [Kolmogorow-Komplexität](def-kolmogorow-complexity) von den verwendeten Modellierungsmitteln, z.B. Programmiersprachen, 'weitestgehend' unabhängig.\n",
    "\n",
    "Durch die Kolmogorow-Komplexität wechseln wir die Perspektive von der *Nachricht* zu ihrer *Konstruktion* durch einen Algorithmus.\n",
    "Wir suchen den kürzesten Algorithmus / die kürzeste Beschreibung für die Erzeugung einer Zeichenkette.\n",
    "Daraufhin analysieren wir nicht die Information selbst, sondern jenen Algorithmus.\n",
    "Je kürzer der Algorithmus, desto geringer der *Informationsgehalt*.\n",
    "\n",
    "Eine Komprimierung ist ein Programm, welches kürzer als der Text ist, welches jenes Programm erzeugt.\n",
    "Aus Sicht der Kolmogorow-Komplexität enthält ein 3000-seitiges Lexikon weniger Informationen als eine gleich lange rein zufällig generierte Zeichenkette, obwohl das Lexikon selbstverständlich viel nützlicher ist.\n",
    "Wir könnten aus den Wörtern des Lexikons bestimmte Zeichenfolgen komprimieren.\n",
    "Für die zufällige Folge an Zeichen ist dies nicht möglich.\n",
    "\n",
    "Auch dieser Begriff bezieht nur die [Syntax](def-syntax) einer vorliegenden *Information* ein.\n",
    "Die [Semantik](def-semantik) wird erneut ausgeklammert."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "source_map": [
   11,
   232,
   234,
   240,
   242,
   253,
   255,
   260,
   262
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}